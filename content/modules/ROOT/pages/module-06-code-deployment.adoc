:imagesdir: ../assets/images
[#code-deployment]
= Building and Deploying an MCP Server
In the last module, you built and ran a fun application using our MaaS model endpoint to power an agentic AI code extension.

Now, we'll take the next step: building and deploying tooling that teaches our model how to understand interact with real-world systems.

Your task: deploy a Model Context Protocol (MCP) server that lets your model interact with your company's Slack workspace.

This is your chance to shape how AI applications are actually built, tested, and deployed — not just in theory, but as part of a larger, evolving platform.

== What's an MCP Server? (The Plain-English version)

Think of MCP servers as plugins for AI. They give you model safe, governed access to things like databases, internal applications, or company Slack instances.

* Without MCP: the model can only talk.
* With MCP: the model can act-fetch info or take approved actions-under platform controls.

== Why Deploy MCP on OpenShift AI?

* Security and Governance: Namespaces, RBAC, and network policies keep tools isolated and controlled.
* Hybrid cloud: Consisten deployment across on-prem, cloud or both.
* Observability: See exactly what ran, where and when.
* Scalability: Roll from a demo pod to a fleet when the use case proves out.

Takeaway: The right AI platform choice determines whether "AI with tools" is a one-off demo or a repeatable, secure enterprise capability. 

== Explore the Deployment with Roo

Open the `llama-stack-on-ocp` -> `slack-mcp` folder. 

image:code/slack-mcp-folder.png[width="50%"]

=== Explain Code

* Click open the `slack-deployment.yaml` file.

image:code/slack_deployment_file.png[width="50%"]

* Highlight the entire file and right-click. Select "Add To Context".

image:code/right-click-popup.png[width="50%"]

You may explore the Roo shortcuts however you'd like. The selected text will be sent to the chat interface of the Roo extension. 

* At the end of the chat window, add the following:

[source,text,role="execute"]
----
Explain the code, do not ask me questions.
----

* Ensure the AI mode is set to `Ask`, as seen below:

image:code/explain_code.png[width="50%"]

* Send Message

image:code/sendmsg.png[width="50%"]

// == Add comments 

//== Add a README.md file

== Deploy the Slack MCP Server

* In the terminal view, paste the following command to deploy the MCP server:

[source,console,role="execute"]
----
oc apply -k /projects/llama-stack-on-ocp/slack-mcp/ -n lls-demo
----

This will create the mcp server deployment and service.

image:code/successful_deploy.png[width="50%"]

== Verify successful deployment

1. In the terminal, run:

[source,console,role="execute"]
----
oc get pods -n lls-demo
----

This will show all pods in the namespace within which we just deployed our slack mcp server. You should see our `slack-mcp-server` pod up and running.

== Wrap Up: What You Did

In this module, you:

* Learned what an MCP server is and why it matters in enterprise AI.

* Deployed an MCP server on OpenShift AI to extend your model’s capabilities.

* Connected the model to a real-world system (Slack) for governed, auditable interaction.

**Why it matters:** You just demonstrated how an AI developer can safely integrate enterprise systems into AI workflows using OpenShift AI — turning a conversational model into an action-capable assistant.