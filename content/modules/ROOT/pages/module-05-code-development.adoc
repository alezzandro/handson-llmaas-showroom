:imagesdir: ../assets/images

[#code-development]
= Building an AI-Powered Application

Welcome to the hands-on development phase of your journey!

In this module, you shift from configuration to creation. As an AI application developer, you'll build a real web application that leverages the AI model infrastructure you've been working with. But here's the twist: you won't be coding alone - you'll be pair programming with an AI assistant.

== The Mission: Build an AI-Powered Task Manager

Your task is to complete a partially-built web application that helps users manage their tasks with intelligent AI assistance. The application can:

* Create, update, and delete tasks with different priority levels
* Mark tasks as completed or reopen them
* Use AI to generate smart suggestions for improving tasks
* Analyze all tasks to provide insights on prioritization and completion timelines

The codebase is intentionally incomplete. Your job is to use **Continue.dev** - the AI coding assistant you configured earlier - to complete the missing functionality.

== Why This Matters

This exercise demonstrates a critical modern development practice: **AI-assisted development**. You're not just learning to write code - you're learning to:

* Articulate what you need in natural language
* Review and validate AI-generated code
* Iterate and refine with AI assistance
* Integrate AI capabilities into real applications

By the end, you'll have built and deployed a working application on OpenShift that itself uses AI - a meta experience that mirrors real-world enterprise development.

== Step 1: Access Your Development Environment

You should already have your OpenShift Dev Spaces environment running from the previous module with Continue.dev configured.

*  If you closed your workspace, navigate back to: https://devspaces.{openshift_cluster_ingress_domain}/[https://devspaces.{openshift_cluster_ingress_domain}/,window=_blank]

* Open your existing workspace by clicking `Open`.

Once inside VSCode, you're ready to start coding.

== Step 2: Import the Workshop Repository to Gitea

Your platform engineer has deployed a local Git server (Gitea) on OpenShift for your team. You'll import the workshop repository from GitHub into your personal Gitea account, where you can make changes and push your completed code.

=== Login to Gitea

* Open Gitea in a new browser tab: {gitea-console-url}[{gitea-console-url},window=_blank]

* Click **Sign In** in the top-right corner

* Log in with your credentials:
** Username: `{user}`
** Password: `{password}`
+
[.bordershadow]
image::code/gitea_login.png[width="50%"]

After logging in, you'll see your personal Gitea dashboard.

=== Create a New Repository by Migrating from GitHub

You'll import the workshop repository from GitHub into your Gitea instance.

* Click the **+** icon in the top-right corner, then select **New Migration**
+
[.bordershadow]
image::code/gitea_new_migration.png[width="50%"]

* Select **GitHub** as the migration source

* Fill in the migration form:
** **Clone Address**: `{git-clone-repo-url}`
** **Owner**: Select your username (`{user}`)
** **Repository Name**: `private-summit-llmaas-showroom`
** Leave other options at their defaults
+
[.bordershadow]
image::code/gitea_migrate_form.png[width="50%"]

* Click **Migrate Repository**

* Wait for the migration to complete (this may take 30-60 seconds)

* You now have your own copy of the repository in Gitea at: `{gitea-console-url}/{user}/private-summit-llmaas-showroom`
+
[.bordershadow]
image::code/gitea_repo_view.png[width="75%"]

=== Clone Your Migrated Repository to Dev Spaces

Now let's get the code into your development environment.

* Return to your **OpenShift Dev Spaces** workspace (VSCode in the browser)

* Open a new terminal in VSCode: **Terminal ‚Üí New Terminal** (or press `Ctrl+Shift+\``)

* Configure Git with your identity (this is required for later commits):
+
[source,bash,role=execute,subs="attributes"]
----
git config --global user.name "{user}"
git config --global user.email "{user}@example.com"
----

* Navigate to the projects directory and clone your forked repository:
+
[source,bash,role=execute,subs="attributes"]
----
cd /projects
git clone {gitea-repo-url}
cd private-summit-llmaas-showroom/exercises/05-code-development
----
+
NOTE: If prompted for credentials, use your username (`{user}`) and password (`{password}`)

* Verify the clone was successful:
+
[source,bash]
----
ls -la
----

* Open this directory in VSCode's file explorer. You should see:
+
----
05-code-development/
‚îú‚îÄ‚îÄ app.py                 # Main Flask application (incomplete)
‚îú‚îÄ‚îÄ requirements.txt       # Python dependencies
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îî‚îÄ‚îÄ index.html        # Frontend UI (complete)
‚îú‚îÄ‚îÄ Dockerfile            # Container configuration
‚îî‚îÄ‚îÄ openshift/            # Kubernetes manifests
    ‚îú‚îÄ‚îÄ buildconfig.yaml
    ‚îú‚îÄ‚îÄ deployment.yaml
    ‚îú‚îÄ‚îÄ imagestream.yaml
    ‚îú‚îÄ‚îÄ route.yaml
    ‚îî‚îÄ‚îÄ service.yaml
----

== Step 3: Review the Incomplete Code

Before jumping into development, let's understand what's missing.

* Open `app.py` in VSCode

* Scroll through the file and notice the functions marked with `# TODO:` comments. These are the functions you need to implement:

** `get_tasks()` - Retrieve all tasks
** `create_task()` - Create a new task
** `update_task()` - Update an existing task
** `delete_task()` - Delete a task
** `suggest_improvements()` - Get AI suggestions for a task
** `analyze_tasks()` - Use AI to analyze all tasks
** `call_ai_model()` - Helper function to call the AI model API

The good news? You have Continue.dev to help you complete these functions!

== Step 4: Complete the Code with Continue.dev

Now comes the exciting part - using AI to complete your application. Let's work through each function systematically.

=== Function 1: Implementing `get_tasks()`

This function should return all tasks sorted by creation date.

* In `app.py`, locate the `get_tasks()` function (around line 23)

* Highlight the entire function including the `# TODO` comment

* Open Continue.dev (click the Continue icon in the left sidebar or press `Ctrl+Shift+L`)

* In the Continue chat, type:
+
----
Complete this function to return all tasks sorted by creation date (newest first). Return as JSON.
----

* Review the generated code. It should:
** Sort tasks by `created_at` in descending order
** Return tasks as JSON using `jsonify()`

* If the code looks correct, accept it by clicking the checkmark or pressing `Tab`

* If you need refinements, ask Continue to adjust: "Add error handling" or "Make sure it returns status 200"

=== Function 2: Implementing `create_task()`

This function creates a new task from JSON input.

* Locate the `create_task()` function (around line 33)

* Highlight the function and ask Continue:
+
----
Complete this function to:
1. Get JSON data from the request
2. Validate that title is provided (return 400 if missing)
3. Create a task with id (use task_counter), title, description, status='pending', created_at (use datetime.now()), and priority
4. Increment task_counter
5. Add task to tasks list
6. Return the created task with status 201
----

* Review the generated code carefully. Make sure it:
** Uses `request.get_json()`
** Validates the title field
** Uses the global `task_counter` variable (includes `global task_counter` declaration)
** Creates a task dictionary with all required fields
** Returns with status code 201

* Accept the code if correct, or iterate with Continue until it's right

=== Function 3: Implementing `update_task()`

* Locate `update_task()` and ask Continue:
+
----
Complete this function to:
1. Find the task by task_id in the tasks list
2. Update only the fields that are provided in the request JSON (title, description, status, priority)
3. Return the updated task as JSON
4. Return 404 with error message if task not found
----

* Verify the implementation handles partial updates correctly

=== Function 4: Implementing `delete_task()`

* Locate `delete_task()` and ask Continue:
+
----
Complete this function to:
1. Find and remove the task from the tasks list by task_id
2. Return success message with status 200
3. Return 404 if task not found
Remember to use 'global tasks'
----

=== Function 5: Implementing `call_ai_model()`

This is a critical function that interfaces with your AI model. Let's be very specific.

* Locate `call_ai_model()` and ask Continue:
+
----
Complete this function to call an OpenAI-compatible chat API:
1. Create payload with: model=AI_MODEL_NAME, messages=[{"role":"user","content":prompt}], max_tokens=max_tokens, temperature=0.7
2. Make POST request to AI_MODEL_URL with JSON payload
3. Extract response from response.json()['choices'][0]['message']['content']
4. Handle requests.exceptions.RequestException and return error message
5. Add timeout of 30 seconds to the request
----

* This is crucial - review carefully to ensure:
** The request payload matches OpenAI's chat completion format
** Error handling is robust
** The function extracts the content correctly

=== Function 6: Implementing `suggest_improvements()`

* Locate `suggest_improvements()` and ask Continue:
+
----
Complete this function to:
1. Create a prompt: "Given this task - Title: {task['title']}, Description: {task['description']}, Priority: {task['priority']} - provide 3 specific, actionable next steps or improvements."
2. Call call_ai_model() with the prompt
3. Return JSON with 'suggestions' field containing the AI response
4. Handle any errors from call_ai_model() and return appropriate error response
----

=== Function 7: Implementing `analyze_tasks()`

The final function brings everything together.

* Locate `analyze_tasks()` and ask Continue:
+
----
Complete this function to:
1. Get analysis_type from request JSON
2. Create prompts based on analysis_type:
   - "priority": "Here are my tasks: {task list}. Which should I prioritize and why?"
   - "completion": "Here are my tasks: {task list}. Estimate how long these will take and suggest an order."
   - "overview": "Here are my tasks: {task list}. Provide a brief overview and insights."
3. Format tasks as a readable list in the prompt
4. Call call_ai_model() with the prompt
5. Return JSON with 'analysis' field
6. Handle errors appropriately
----

== Step 5: Test Your Code Locally

Before deploying to OpenShift, let's test locally in your Dev Spaces environment.

* In the terminal, make sure you're in the exercise directory:
+
[source,bash]
----
cd /projects/private-summit-llmaas-showroom/exercises/05-code-development
----

* Install dependencies:
+
[source,bash]
----
pip install -r requirements.txt
----

* Set environment variables to point to your AI model. Replace `{user}` with your actual username:
+
[source,bash,subs="attributes"]
----
export AI_MODEL_URL="http://granite-instruct-vllm.{user}-ai-models.svc.cluster.local:8000/v1/chat/completions"
export AI_MODEL_NAME="granite-3.0-8b-instruct"
export PORT=8080
----

* Run the application:
+
[source,bash]
----
python app.py
----

* You should see output like:
+
----
 * Running on http://0.0.0.0:8080
 * Debug mode: on
----

* In Dev Spaces, you might see a popup offering to open the port. Click **Open in New Tab** or manually navigate to the port forwarding URL shown in the terminal.

* Test the application:
** Create a few tasks with different priorities
** Try marking tasks as completed
** Click "AI Suggestions" on a task to test the AI integration
** Try the different analysis options

* If you encounter errors:
** Check the terminal for Python errors
** Use Continue to help debug: Copy the error message and ask "What's wrong with this error?"
** Verify your environment variables are set correctly

* Once everything works, stop the application with `Ctrl+C`

== Step 6: Prepare for OpenShift Deployment

Now let's get your application ready to deploy on OpenShift.

* First, create a new project (namespace) for your application in OpenShift. In the terminal:
+
[source,bash,subs="attributes"]
----
oc login --insecure-skip-tls-verify=true -u {user} -p {password} {openshift_api_url}
oc new-project {user}-task-manager
----

* Update the deployment configuration with your namespace. Open `openshift/deployment.yaml` in VSCode

* Find the line with `image:` (around line 17) and replace `NAMESPACE` with `{user}-task-manager`:
+
[source,yaml,subs="attributes"]
----
image: image-registry.openshift-image-registry.svc:5000/{user}-task-manager/task-manager:latest
----

* Also update the AI model URL in the `env` section (around line 23). Replace it with:
+
[source,yaml,subs="attributes"]
----
- name: AI_MODEL_URL
  value: "http://granite-instruct-vllm.{user}-ai-models.svc.cluster.local:8000/v1/chat/completions"
----

* Save the file

* Now open `openshift/buildconfig.yaml` in VSCode

* Find the `uri:` line (around line 17) and update it with your Gitea repository URL:
+
[source,yaml,subs="attributes"]
----
uri: {gitea-repo-url}
----
+
This tells OpenShift to build from your personal Gitea repository instead of the upstream GitHub repository.

* Save the file

* Commit these configuration changes to your Gitea repository:
+
[source,bash,subs="attributes"]
----
cd /projects/private-summit-llmaas-showroom
git add exercises/05-code-development/openshift/deployment.yaml
git add exercises/05-code-development/openshift/buildconfig.yaml
git commit -m "Configure deployment for {user}"
git push origin main
----
+
NOTE: If prompted for credentials during push, use your username (`{user}`) and password (`{password}`)

== Step 7: Commit Your Completed Code

Before deploying to OpenShift, you need to commit and push your completed application code to your Gitea repository. This ensures OpenShift builds the latest version of your code.

* Make sure you're in the repository root directory:
+
[source,bash]
----
cd /projects/private-summit-llmaas-showroom
----

* Check which files you've modified:
+
[source,bash]
----
git status
----
+
You should see `exercises/05-code-development/app.py` as modified.

* Stage and commit your completed code:
+
[source,bash,subs="attributes"]
----
git add exercises/05-code-development/app.py
git commit -m "Complete task manager application with AI integration"
----

* Push your changes to Gitea:
+
[source,bash]
----
git push origin main
----
+
NOTE: If prompted for credentials, use your username (`{user}`) and password (`{password}`)

* Verify the push was successful - you can check your Gitea repository in the browser to see the updated code

Now your completed application is ready to be built and deployed!

== Step 8: Build and Deploy to OpenShift

With your code complete, tested, and pushed to Gitea, it's time to deploy!

=== Create the ImageStream

An ImageStream tracks image updates in OpenShift.

[source,bash]
----
oc apply -f openshift/imagestream.yaml
----

=== Create the Build Configuration

This tells OpenShift how to build your container image from source.

[source,bash]
----
oc apply -f openshift/buildconfig.yaml
----

=== Start the Build

Trigger a build from your Gitea repository:

[source,bash]
----
oc start-build task-manager --follow
----

This will:

* Clone the repository
* Build a container image using your Dockerfile
* Push the image to OpenShift's internal registry

The build will take 2-3 minutes. Watch for `Push successful` at the end.

=== Deploy the Application

Now deploy your application:

[source,bash]
----
oc apply -f openshift/deployment.yaml
oc apply -f openshift/service.yaml
oc apply -f openshift/route.yaml
----

=== Verify the Deployment

Check that your pod is running:

[source,bash]
----
oc get pods
----

You should see something like:

----
NAME                            READY   STATUS    RESTARTS   AGE
task-manager-7d9f4c5b6d-x8h2k   1/1     Running   0          30s
----

If the status shows `ImagePullBackOff` or `CrashLoopBackOff`, check the logs:

[source,bash]
----
oc logs -f deployment/task-manager
----

=== Get Your Application URL

Find your application's public URL:

[source,bash]
----
oc get route task-manager -o jsonpath='{.spec.host}'
----

Copy this URL and open it in your browser!

== Step 9: Test Your Deployed Application

Your AI-powered task manager is now live on OpenShift!

* Open the application URL in your browser

* Create several tasks:
** "Implement user authentication" (High priority)
** "Write unit tests" (Medium priority)
** "Update documentation" (Low priority)

* Click **AI Suggestions** on the authentication task
** You should see intelligent, specific suggestions from the AI model

* Try the analysis features:
** Click "üìä Prioritize Tasks" - AI will suggest which tasks to tackle first
** Click "üìã Task Overview" - Get a summary of all your tasks
** Click "‚è±Ô∏è Completion Estimate" - AI will estimate timelines

* Mark some tasks as completed and observe how the UI updates

== Step 10: Review the OpenShift Console

Let's see your application from the platform perspective.

* Open the OpenShift Console: https://console-openshift-console.{openshift_cluster_ingress_domain}/[https://console-openshift-console.{openshift_cluster_ingress_domain}/,window=_blank]

* Navigate to **Developer** perspective (top-left dropdown)

* Select your project: `{user}-task-manager`

* Click on **Topology** to see a visual representation of your application

You should see:

* Your `task-manager` deployment (represented as a blue circle)
* The route (indicated by an arrow icon in the top-right of the deployment)
* Build information if you click on the deployment

* Click on the deployment circle to open the details panel:
** **Resources** tab shows your pod, service, and route
** **Pods** section shows if your pod is healthy
** **Builds** section shows your build history

* Click **View logs** on your pod to see the application logs in real-time

== Step 11: Understanding the Architecture

Let's reflect on what you've built and how it works:

=== Application Architecture

Your task manager consists of:

* **Frontend**: A single-page application built with vanilla JavaScript and HTML
* **Backend**: Flask REST API with endpoints for CRUD operations
* **AI Integration**: Direct API calls to your Granite model deployed in Module 1

=== OpenShift Components

Your deployment uses:

* **BuildConfig**: Defines how to build your container from source
* **ImageStream**: Tracks your container image versions
* **Deployment**: Manages your application pods with health checks
* **Service**: Provides internal networking for your pods
* **Route**: Exposes your application to the internet with TLS

=== AI Integration Flow

When a user requests AI suggestions:

1. Frontend sends POST request to `/api/tasks/<id>/suggest`
2. Flask backend retrieves the task details
3. Backend constructs a prompt with task information
4. Backend calls `call_ai_model()` which sends a request to Granite
5. Granite processes the prompt and returns suggestions
6. Backend returns suggestions to frontend
7. Frontend displays the AI-generated suggestions

This is a **synchronous** AI integration pattern - simple but effective for low-latency requests.

== Reflection: What You've Accomplished

Let's take a moment to appreciate what you've achieved:

‚úÖ **AI-Assisted Development**: You used Continue.dev to complete a real application, experiencing modern development workflows

‚úÖ **Full-Stack Development**: You worked with Python, Flask, JavaScript, HTML, and REST APIs

‚úÖ **AI Integration**: You integrated an LLM into an application, handling prompts, API calls, and responses

‚úÖ **Container Building**: You created a production-ready Dockerfile for your application

‚úÖ **OpenShift Deployment**: You deployed to a production-grade Kubernetes platform with proper builds, deployments, and routing

‚úÖ **Cloud-Native Practices**: You used health checks, environment variables, and 12-factor app principles

This mirrors real enterprise development: AI models aren't just standalone services - they're integrated into applications that solve business problems.

== Optional Challenges

Want to go further? Try these enhancements:

=== Challenge 1: Add Task Persistence

Right now, tasks are stored in memory and disappear when the pod restarts.

* Ask Continue.dev: "How can I add SQLite database persistence to this Flask app?"
* Implement a simple database backend
* Redeploy and verify tasks persist across pod restarts

=== Challenge 2: Enhance the AI Prompts

The AI suggestions could be more sophisticated.

* Modify the prompts in `suggest_improvements()` to include more context
* Try different temperature values for creative vs. consistent responses
* Add system messages to guide the AI's behavior

=== Challenge 3: Add Authentication

Protect your task manager with user authentication.

* Use Continue.dev to add basic Flask-Login authentication
* Create user registration and login pages
* Associate tasks with specific users

=== Challenge 4: Improve Error Handling

Make the application more robust.

* Add try-except blocks around all API calls
* Return user-friendly error messages
* Add retry logic for AI model calls

== Troubleshooting

=== Application Won't Start

* Check pod logs: `oc logs deployment/task-manager`
* Verify environment variables in `deployment.yaml`
* Ensure the image built successfully: `oc get builds`

=== AI Suggestions Don't Work

* Verify the AI model service is accessible from your namespace
* Check the `AI_MODEL_URL` environment variable
* Test the model endpoint directly:
+
[source,bash,subs="attributes"]
----
oc exec deployment/task-manager -- curl -X POST http://granite-instruct-vllm.{user}-ai-models.svc.cluster.local:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"granite-3.0-8b-instruct","messages":[{"role":"user","content":"Hello"}],"max_tokens":50}'
----

=== Build Failures

* Check build logs: `oc logs bc/task-manager`
* Verify the BuildConfig points to the correct repository and branch
* Ensure the `contextDir` is set to `exercises/05-code-development`

== What's Next?

You've successfully built and deployed an AI-powered application on OpenShift! You've experienced:

* The power of AI-assisted development with Continue.dev
* Full-stack application development with AI integration
* Modern DevOps practices with OpenShift

In the next modules, you'll explore even more advanced AI capabilities, including agentic AI systems that can perform complex multi-step tasks.

Your task manager is just the beginning - imagine what else you can build with these tools!
