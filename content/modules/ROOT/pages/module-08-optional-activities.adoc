= [Optional] Additional Exercises

[#deploying-a-model]
== Deploying a new Model

You are now ready to deploy you own instance of the `TinyLlama` model. This model is a much smaller version of the `Llama` model that can be used on a CPU. We chose it for this exercise as it's way faster to deploy than the `Granite` model, and does not require another expensive GPU.

We have seen through `ODH-TEC` that the model is already available in our bucket, and that the existing connection already points to this bucket. So we are going to deploy `TinyLlama` using the same connection as the `Granite` model.

=== Deploying the Model

* In the Dashboard, navigate to the "Models" tab.
* Click on the `Deploy model` button to start the model deployment process.
+
[.bordershadow]
image::02/02-deploy-model-button.png[]

* In the deployment form, fill in the following details:
** **Model deployment name**: `TinyLlama`
** **Serving runtime**: `Custom - vLLM ServingRuntime-CPU`
** **Number of model server replicas to deploy**: `1`
** **Model server size**: `Medium` **(IMPORTANT)**
** **Accelerator**: `None`
** **Model route**: Checked **(IMPORTANT)**
** **Token authentication**: Unchecked **(IMPORTANT)**
** **Source model location**: `Existing connection`
** **Connection**: `models`
** **Path**: `TinyLlama/TinyLlama-1.1B-Chat-v1.0`
** **Additional serving runtime arguments**:
+
[source,bash,role="execute",subs="+macros,+attributes"]
----
--served-model-name=tinyllama/tinyllama-1.1b-chat-v1.0
----

* Then click on `Deploy` to start the deployment process. This will take a few minutes to complete. At any time, you can check the status of the deployment by opening the OpenShift console and navigating to the `llm-hosting` project. You should see a new Pod called `tinyllama-predictor-...` being deployed. Looking at its logs, you should see the model being loaded and the server starting.
* Once the deployment is complete, click on `Internal and external endpoint details` to see the different endpoints available for the model.
+
[.bordershadow]
image::02/02-new-model-endpoints.png[]

=== Testing the Model

To test the model, we are going to use the `TinyLlama` model endpoint. You can use any HTTP client to test the model, but we will use `curl` for this workshop.

* To do so, we will first create a new Workbench based on VSCode. On the `Workbench` tab, click on `Create workbench`.
+
[.bordershadow]
image::02/02-create-workbench.png[]

* Create your workbench with the following parameters:
** **Name**: `Model-Test`
** **Image selection**: `code-server`
** **Version selection**: `2024.2`
** **Deployment size**: Standard (default and only option in this cluster)
** **Accelerator**: None
** **Persistent storage size**: 5 GiB (default)

* See screenshot below for reference:

+
[.bordershadow]
image::02/02-create-workbench-full-1.png[width=800]
+
[.bordershadow]
image::02/02-create-workbench-full-2.png[width=800]

* Then click on `Create workbench` at the bottom.
+
[.bordershadow]
image::02/02-create-workbench-button.png[]

* Your new workbench should appear in the list of workbenches in Starting mode. After a few moments, you can click on `Open` to access it.
* After logging in, you will access the VSCode interface. You can close the Welcome tab if you want.
* Open a Terminal in VSCode by clicking on the `Terminal` menu, then `New Terminal`. You can also use the shortcut `Ctrl + Shift + \`` (the backtick key).
+
[.bordershadow]
image::02/02-open-terminal.png[]

* In the terminal, you can use `curl` to test the model. To test the internal endpoint, run the following command:
+
[source,bash,role="execute",subs="+macros,+attributes"]
----
curl -k -X POST -H "Content-Type: application/json" -d '{"model":"tinyllama/tinyllama-1.1b-chat-v1.0" ,"prompt": "Hello, how are you?"}' https://tinyllama.llm-hosting.svc.cluster.local/v1/completions | jq
----
+
NOTE: When pasting content for the first time, your workbench may prompt you to grant paste access. Please allow it to enable copy-paste functionality.

** To test the external endpoint, run the following command:
+
[source,bash,role="execute",subs="+macros,+attributes"]
----
curl -X POST -H "Content-Type: application/json" -d '{"model":"tinyllama/tinyllama-1.1b-chat-v1.0" ,"prompt": "Hello, how are you?"}' https://tinyllama-llm-hosting.{openshift_cluster_ingress_domain}/v1/completions | jq
----

In both cases, the answer should take some more time than previously as we are running on a CPU (and you may need to try a few times to get something meaningful as the model has to be "warmed-up"), and look like this:

[source,json,role="execute",subs="+macros,+attributes"]
----
{
    "id":"cmpl-061bbe3b71394ef98765f44e93c0df9b",
    "object":"text_completion",
    "created":1746042889,
    "model":"tinyllama/tinyllama-1.1b-chat-v1.0",
    "choices":[
        {
            "index":0,
            "text":" I'm well. My name is David and I live in Tokyo. Rec",
            "logprobs":null,
            "finish_reason":"length",
            "stop_reason":null,"prompt_logprobs":null
        }
        ],
    "usage":{
        "prompt_tokens":6,
        "total_tokens":22,
        "completion_tokens":16
        }
}
----

Congratulations, you have successfully deployed and tested a model on OpenShift AI!

You can now use this model in your applications, or expose it through the 3Scale API Gateway to make it available to other users. 