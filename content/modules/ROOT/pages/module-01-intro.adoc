= [Module 1] Setting Up Model-as-a-Service Infrastructure

== Introduction: The Cost of Infrastructure-as-a-Service

Self-service infrastructure access is good when you have plentiful resources and small-scale teams...however:

- Very few people know how to use these resources correctly
- Throwing GPUs at a problem can be risky
- Leads to duplication and resource under-utilization
- Leads to high infrastructure costs
- Most people want an LLM endpoint, not a GPU for themselves.

[.bordershadow]
image::../assets/images/02/iaas.png[]

== Models-as-a-Service

Offering AI models as a service to a larger audience, especially Large Language Models (LLMs), solves many of these problems:

* IT serves common models centrally
** Generative AI focus, applicable to any model
** Centralized pool of hardware 
** Platform Engineering for AI
** AI management (versioning, regression testing, etc)
* Models available through API Gateway
* Developers consume models, build AI applications
** For end users (private assistants, etc) 
** To improve products or services through AI 
* Shared Resources business model keeps costs down


[.bordershadow]
image::../assets/images/02/maas.png[]

== Module Goals

In this first module, you will:

* Get an overview of the OpenShift AI platform
* Deploy a Generative AI model on OpenShift AI
* Configure 3Scale API Gateway to expose the model to end users

At the end of the module, you will have learned how to deploy and manage models on OpenShift AI, especially LLMs, and make them available to a whole organization in a secure, private, and efficient way.