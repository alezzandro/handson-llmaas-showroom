:imagesdir: ../assets/images
[#agentic-ai]
= System Administration with Agentic AI 

In the last module, you stepped into the developer role building applications using private LLM endpoints. Now, you'll become a site reliability engineer (SRE) or DevOps practitioner focused on observability, diagnostics, and operational scale. 

But you won't go at it alone, you'll work with agentic AI tooling to navigate your OpenShift environment in real time - just like a modern SRE team building insight-driven automation.

== Why Agentic AI for Admins?

Think of agentic AI as your copilot for cluster operations - ready to answer complex infrastructure questions using real-time telemetry and reasoning. 

Instead of complex dashboards, YAML, and shell scripts, you'll try something new:

* “What’s consuming the most memory in our dev cluster right now?”

* “Were there any pod crashes in the last 30 minutes?”

* “Why did that Job fail this morning?”

These natural-language prompts get translated into real queries and results. The outcome? Faster issue resolution, increased system transparency, and lower barrier to operational insights for both technical and non-technical users.

That's an example of the power of agentic AI.

== Meet Your New Toolkit

In this module, we'll use a preconfigured stack that includes:

* https://github.com/meta-llama/llama-stack[LlamaStack] - a flexible open-source framework developed by Meta to simplify the creation and deployment of advanced AI applications.
** LlamaStack provides a unified API layer for inference, RAG, agents, tools, safety, evals and telemetry.
** **Why It Matters for Enterprises**:
*** Codifies best practices across Gen AI tools
*** Enables reproducible, explainable, and extensible AI workflows
** In short, it lets you focus on value creation, not toolchain assembly.

IMPORTANT: You could also use frameworks like LangChain or CrewAI instead of LlamaStack with OpenShift AI. All of these tools help you build agentic AI workflows with reasoning, tool use, and orchestration.

* **OpenShift MCP Server**  - allows LLM agents to interact with the live OpenShift cluster through a standardized, read-only interface

* https://huggingface.co/meta-llama/Llama-Guard-3-1B[Llama Guard] - optional input/output filtering for responsible AI interactions using another safety-trained LLM.

== Connect Your Model to LlamaStack

If you did not save your MaaS application URL, navigate back to the 3Scale developer portal to grab it:

Developer Portal: https://maas.{openshift_cluster_ingress_domain}[https://maas.{openshift_cluster_ingress_domain},window=_blank].

=== Add Model Endpoint to LlamaStack Distribution File

1. Go back to the OpenShift console: 

https://console-openshift-console.{openshift_cluster_ingress_domain}/[https://console-openshift-console.{openshift_cluster_ingress_domain}/,window=_blank].

2. In the Administrator perspective, select API Explorer.

image:llama/api_explorer.png[width="50%"]

3. Search `llamastackdistribution` in the search bar and select the resource.

image:llama/llamastackdistribution.png[width="50%"]

4. Select `Instances` and the available instance.

image:llama/llamastackinstance.png[width="50%"]

5. Select `YAML` and scroll down to the highlighted section of text.

image:llama/lsd_yaml.png[width="50%"]

6. In place of the existing Granite URL, input your endpoint URL from the 3scale developer portal. Ensure `/v1` is appended to the string.

image:llama/maas_endpoint.png[width="50%"]

7. Click save

image:llama/save_yaml.png[width="50%"]

== View Your Deployment

1. Select the `Developer` perspective.

image:llama/dev_perspective.png[width="50%"]

2. Search for the `lls-demo` namespace

image:llama/find-namespace.png[width="50%"]

In the Topology view, you will see three pods:

* **LlamaStack**: core server.
* **OCP MCP Server**: an MCP Server with tools to help our model interact with and understand OpenShift.
* **LlamaStack Playground**: A streamlit UI to interact with the system.

Feel free to poke around and explore the deployment.

3. Select the LlamaStack playground hyperlink to open the UI.

image:llama/playground_link.png[width="50%"]

Now you will see the "playground" user interface. This application was created in the upstream project for the purposes of demonstration and experimentation and is not a supported component of our downstream OpenShift AI product.

== Configure the AI Agent

Within the application you'll find a familiar chat interface with some selection options on the left-hand side.

1. Select our model from the drop down

[.bordershadow]
image::llama/model_selection.png[width="50%"]

2. Set `Processing mode` -> `Agent-based`, giving us access to tools.

image::llama/agent_selection.png[width="50%"]

3. Enable the OpenShift MCP tool group.

image::llama/mcp_server.png[width="50%"]

4. Once the MCP server is selected, you can peruse the active tools available.

image:llama/active_tools.png[width="50%"]

Everything else can remain with the default settings. You can now query live cluster data using plain English.

== Try It Out

The active tools information will give you guidance into how to interact with the model in chat to activate the tool calls correctly.

NOTE: Our LlamaStack deployment is namespace-scoped. Therefore, in this activity, we will only be able to interact with the resources within the `lls-demo` namespace containing the LlamaStack server and playground.

In the chat, enter:

[source,console,role=execute,subs=attributes+]
----
Get pods in the lls-demo namespace
----

Try a few more:

[source,console,role=execute,subs=attributes+]
----
Get deployment resources in lls-demo namespace
----

Feel free to experiment!

NOTE: The provided MCP server is experimental for demo purposes. Some responses may be incomplete or inconssitent, and the model may hallucinate or misinterpret results if the tool output is vague or malformed. The demonstration is meant to highlight the potential of natural language interfaces for interacting with infrastructure, and how emerging tools like LlamaStack and MCP can reduce the barrier to entry for understanding system behavior and save valuable time and effort.

// TO DO: === Query Deployed Job

// Query related to the activity of last module which should have succeeded in a deployment

// === Engage with slack workspace connection

=== Add Responsible AI Shields

To enforce guardrails on inputs and outputs, select the **Llama Guard** model under the `Input Shields` and `Output Shields` form fields:

image::llama/guards.png[width="50%"]

This helps to filter inappropriate prompts and responses.

== Summary: What You Did

In this module, you:

* Acted as an SRE or DevOps practitioner using AI for cluster resource insight
* Integrated your own LLM with a tool-using agent.
* Explored OpenShift resources with natural language
* Added AI guardrails with input/output shields.

You just used AI to reduce operational complexity and speed up workflows! 