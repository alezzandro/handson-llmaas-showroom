:imagesdir: ../assets/images

[#code-development]
= Building an AI-Powered Application

Welcome to the hands-on development phase of your journey!

In this module, you shift from configuration to creation. As an AI application developer, you'll build a real web application that leverages the AI model infrastructure you've been working with. But here's the twist: you won't be coding alone - you'll be pair programming with an AI assistant.

== The Mission: Build an AI-Powered Task Manager

Your task is to complete a partially-built web application that helps users manage their tasks with intelligent AI assistance. The application can:

* Create, update, and delete tasks with different priority levels
* Mark tasks as completed or reopen them
* Use AI to generate smart suggestions for improving tasks
* Analyze all tasks to provide insights on prioritization and completion timelines

The codebase is intentionally incomplete. Your job is to use **Continue.dev** - the AI coding assistant you configured earlier - to complete the missing functionality.

== Why This Matters

This exercise demonstrates a critical modern development practice: **AI-assisted development**. You're not just learning to write code - you're learning to:

* Articulate what you need in natural language
* Review and validate AI-generated code
* Iterate and refine with AI assistance
* Integrate AI capabilities into real applications

By the end, you'll have built and deployed a working application on OpenShift that itself uses AI - a meta experience that mirrors real-world enterprise development.

== Step 1: Access Your Development Environment

You should already have your OpenShift Dev Spaces environment running from the previous module with Continue.dev configured and the workshop repository cloned.

*  If you closed your workspace, navigate back to: https://devspaces.{openshift_cluster_ingress_domain}/[https://devspaces.{openshift_cluster_ingress_domain}/,window=_blank]

* Open your existing workspace by clicking `Open`.

* On the VSCode's file explorer you should see:
+
----
handson-llmaas-code-development
‚îú‚îÄ‚îÄ openshift              # Kubernetes manifests
‚îú‚îÄ‚îÄ templates              # Frontend UI (complete)
‚îú‚îÄ‚îÄ app.py                 # Main Flask application (incomplete)
‚îú‚îÄ‚îÄ Dockerfile             # Container configuration
‚îî‚îÄ‚îÄ requirements.txt       # Python dependencies
----

== Step 2: Review the Incomplete Code

Before jumping into development, let's understand what's missing.

* Open `app.py` in the VSCode file explorer

* Scroll through the file and notice the functions marked with `# TODO:` comments. These are the functions you need to implement:

** `get_tasks()` - Retrieve all tasks
** `create_task()` - Create a new task
** `update_task()` - Update an existing task
** `delete_task()` - Delete a task
** `suggest_improvements()` - Get AI suggestions for a task
** `analyze_tasks()` - Use AI to analyze all tasks
** `call_ai_model()` - Helper function to call the AI model API

The good news? You have Continue.dev to help you complete these functions!

== Step 3: Complete the Code with Continue.dev

Now comes the exciting part - using AI to complete your application. Let's work through each function systematically.

**How to Use Continue for Code Completion:**

Instead of letting Continue edit your code automatically, you'll use a more controlled approach:

1. **Highlight** the entire function (including the `# TODO` comments)
2. **Right-click** ‚Üí **Continue** ‚Üí **"Add to chat"**
3. **Review** the code and explanation Continue provides in the chat window
4. **Copy** the relevant code from the chat response
5. **Paste** it into your actual code file, replacing the TODO section

This approach gives you better control and helps you understand the code before adding it.

=== Function 1: Implementing `get_tasks()`

This function should return all tasks sorted by creation date.

* In `app.py`, locate the `get_tasks()` function

* **Highlight the entire function** from the `@app.route` decorator through the `pass` statement, including all the TODO comments

* **Right-click** on the highlighted code ‚Üí **Continue** ‚Üí **"Add to chat"** (or press `Ctrl-L`)

* In the Continue chat window that opens, copy the following text:
+
[source,role="execute"]
----
Complete this function to return all tasks sorted by creation date (newest first). Return as JSON.

Remember: 'tasks' is a global list variable defined at the top of the file.
----
and then hit `Enter` or press the `Send` button.

image::code/chat_complete_get_tasks.png[width="50%"]

* Continue will generate the code in the chat window. **Review the generated code.** It should:
** Sort tasks by `created_at` in descending order
** Return tasks as JSON using `jsonify()`
** Return status code 200

* **Copy the implementation code** from the chat (the part inside the function body). You can also use the `Apply` button and you will be asked to accept the changes in `app.py`.

* **Paste it into your `app.py` file**, replacing the TODO comments, `sorted_tasks` and `pass` statement

* If you need refinements, continue chatting: "Add error handling" or "Make sure it returns status 200"

=== Function 2: Implementing `create_task()`

This function creates a new task from JSON input.

* Locate the `create_task()` function

* **Highlight the entire function** from `@app.route` through `pass`

* **Right-click** ‚Üí **Continue** ‚Üí **"Add to chat"**

* In the Continue chat, type:
+
[source,role="execute"]
----
Complete this function to:
1. Get JSON data from the request
2. Validate that title is provided (return 400 if missing)
3. Create a task with id (use task_counter), title, description, status='pending', created_at (use datetime.now()), and priority
4. Increment task_counter
5. Add task to tasks list
6. Return the created task with status 201

Remember: 'tasks' is a global list variable and 'task_counter' is a global integer variable defined at the top of the file. You must declare 'global task_counter' at the beginning of the function to modify it.
----

* **Review the generated code** carefully. Make sure it:
** Uses `request.get_json()`
** Validates the title field
** Uses the global `task_counter` variable (includes `global task_counter` declaration)
** Creates a task dictionary with all required fields
** Returns with status code 201

* **Copy and paste** the implementation into your `app.py` file

* If needed, iterate with Continue in the chat until it's right

=== Function 3: Implementing `update_task()`

* Locate the `update_task()` function

* **Highlight the entire function** from `@app.route` through `pass`

* **Right-click** ‚Üí **Continue** ‚Üí **"Add to chat"**

* In the Continue chat, type:
+
[source,role="execute"]
----
Complete this function to:
1. Find the task by task_id in the tasks list
2. Update only the fields that are provided in the request JSON (title, description, status, priority)
3. Return the updated task as JSON
4. Return 404 with error message if task not found

Remember: 'tasks' is a global list variable defined at the top of the file. You need to iterate through it to find the task with matching task_id.
----

* **Review the code** and verify the implementation handles partial updates correctly

* **Copy and paste** the implementation into your `app.py` file

=== Function 4: Implementing `delete_task()`

* Locate the `delete_task()` function

* **Highlight the entire function** from `@app.route` through `pass` (note: the `global tasks` line is already there)

* **Right-click** ‚Üí **Continue** ‚Üí **"Add to chat"**

* In the Continue chat, type:
+
[source,role="execute"]
----
Complete this function to:
1. Find and remove the task from the tasks list by task_id
2. Return success message with status 200
3. Return 404 if task not found

Remember: 'tasks' is a global list variable defined at the top of the file. The 'global tasks' declaration is already in the function, so you can directly modify the list.
----

* **Review the code** in the chat

* **Copy and paste** the implementation into your `app.py` file

=== Function 5: Implementing `suggest_improvements()`

* Locate the `suggest_improvements()` function

* **Highlight the entire function** from `@app.route` through `pass`, including the task-finding logic that's already there

* **Right-click** ‚Üí **Continue** ‚Üí **"Add to chat"**

* In the Continue chat, type:
+
[source,role="execute"]
----
Complete this function to:
1. Create a prompt: "Given this task - Title: {task['title']}, Description: {task['description']}, Priority: {task['priority']} - provide 3 specific, actionable next steps or improvements."
2. Call call_ai_model() with the prompt
3. Return JSON with 'suggestions' field containing the AI response
4. Handle any errors from call_ai_model() and return appropriate error response

Note: The task-finding logic is already implemented in the function (it searches through the 'tasks' global list). You only need to complete the TODO section for creating the prompt and calling the AI.
----

* **Review the code** and ensure it properly calls the `call_ai_model()` function

* **Copy and paste** the implementation into your `app.py` file

=== Function 6: Implementing `analyze_tasks()`

The final function brings everything together.

* Locate the `analyze_tasks()` function

* **Highlight the entire function** from `@app.route` through `pass`, including all TODO comments

* **Right-click** ‚Üí **Continue** ‚Üí **"Add to chat"**

* In the Continue chat, type:
+
[source,role="execute"]
----
Complete this function to:
1. Get analysis_type from request JSON
2. Create prompts based on analysis_type:
   - "priority": "Here are my tasks: {task list}. Which should I prioritize and why?"
   - "completion": "Here are my tasks: {task list}. Estimate how long these will take and suggest an order."
   - "overview": "Here are my tasks: {task list}. Provide a brief overview and insights."
3. Format tasks as a readable list in the prompt
4. Call call_ai_model() with the prompt
5. Return JSON with 'analysis' field
6. Handle errors appropriately

Remember: 'tasks' is a global list variable defined at the top of the file. You need to iterate through it to build a formatted task list for the AI prompt.
----

* **Review the code** to ensure it handles all three analysis types correctly

* **Copy and paste** the implementation into your `app.py` file

=== Function 7: Implementing `call_ai_model()`

This is a critical function that interfaces with your AI model. Let's be very specific.

* Locate the `call_ai_model()` function (around line 129)

* **Highlight the entire function** from `def call_ai_model` through `pass`, including all the docstring and TODO comments

* **Right-click** ‚Üí **Continue** ‚Üí **"Add to chat"**

* In the Continue chat, type:
+
[source,role="execute"]
----
Complete this function to call an OpenAI-compatible chat API:
1. Get AI_API_TOKEN from environment variables (os.environ.get('AI_API_TOKEN'))
2. Create headers with: {"Content-Type": "application/json", "Authorization": f"Bearer {api_token}"}
3. Create payload with: model=AI_MODEL_NAME, messages=[{"role":"user","content":prompt}], max_tokens=max_tokens, temperature=0.7
4. Make POST request to AI_MODEL_URL with JSON payload and headers
5. Extract response from response.json()['choices'][0]['message']['content']
6. Handle requests.exceptions.RequestException and return error message
7. Add timeout of 30 seconds to the request

Remember: AI_MODEL_URL, AI_MODEL_NAME, and AI_API_TOKEN are global configuration variables defined at the top of the file. Use them directly in this function.
----

* **This is crucial - review carefully** to ensure:
** The request includes the Authorization header with the API token
** The request payload matches OpenAI's chat completion format
** Error handling is robust
** The function extracts the content correctly

* **Copy and paste** the implementation into your `app.py` file


Congratulations! You've now completed all the functions using Continue.dev!

== Step 4: Test Your Code Locally

Before deploying to OpenShift, let's test locally in your Dev Spaces environment.

* In the terminal, make sure you're in the exercise directory:
+
[source,bash,role="execute"]
----
cd /projects/handson-llmaas-code-development
----

* Install dependencies:
+
[source,bash,role="execute"]
----
pip install -r requirements.txt
----

* Set environment variables to point to your AI model.
+
[source,bash,role="execute"]
----
export AI_MODEL_URL="https://granite-33-8b-instruct.llm-hosting.svc.cluster.local/v1/chat/completions"
export AI_MODEL_NAME="granite-3.3-8b-instruct"
export AI_API_TOKEN="your-api-token-here"
export PORT=8080
----
+
NOTE: Replace `your-api-token-here` with your actual API token from the MaaS platform (the same token you used in the Continue configuration in xref:module-02-01-code-assistant.adoc#code-asst[Module 2.1: Coding with AI])

* Run the application:
+
[source,bash,role="execute"]
----
python3.11 app.py
----

* You should see output like:
+
----
 * Running on http://0.0.0.0:8080
 * Debug mode: on
----

* In Dev Spaces, you might see a popup offering to open the port. Click **Open in New Tab** or manually navigate to the port forwarding URL shown in the terminal.

* Test the application:
** Create a few tasks with different priorities
** Try marking tasks as completed
** Click "AI Suggestions" on a task to test the AI integration
** Try the different analysis options

* If you encounter errors:
** Check the terminal for Python errors
** Use Continue to help debug: Copy the error message and ask "What's wrong with this error?"
** Verify your environment variables are set correctly

* Once everything works, stop the application with `Ctrl+C`

== Step 5: Prepare for OpenShift Deployment

Now let's get your application ready to deploy on OpenShift.

* First, open a terminal in your OpenShift Dev Spaces workspace. The workspace terminal comes pre-configured with OpenShift CLI (`oc`) and is automatically logged into your OpenShift cluster, so you don't need to run `oc login`.

* To open a terminal in Dev Spaces:
+
1. In the top menu bar, click on **Terminal**
2. Select **New Terminal**
+
image::02/02-open-terminal.png[width="50%"]
+
TIP: The Dev Spaces workspace is already authenticated with OpenShift using your credentials, so you can immediately start using `oc` commands.

* Now create a new project (namespace) for your application. In the terminal:
+
[source,bash,subs="attributes"]
----
oc new-project {user}-task-manager
----

* Update the deployment configuration with your AI model details. Open `openshift/deployment.yaml` in VSCode

* Find the AI model configuration in the `env` section (around line 28) and update it with your namespace:
+
[source,yaml,subs="attributes"]
----
- name: AI_MODEL_URL
  value: "https://granite-33-8b-instruct.llm-hosting.svc.cluster.local/v1/chat/completions"
- name: AI_MODEL_NAME
  value: "granite-3.3-8b-instruct"
- name: AI_API_TOKEN
  value: "your-api-token-here"
----
+
NOTE: Replace `your-api-token-here` with your actual API token from the MaaS platform. The deployment already references the image stream, so no namespace changes are needed for the image.

* Save the file

* Now open `openshift/buildconfig.yaml` in VSCode

* Find the `uri:` line (around line 17) and update it with your Gitea repository URL:
+
[source,yaml,subs="attributes"]
----
uri: https://gitea-gitea.{openshift_cluster_ingress_domain}/studentX/handson-llmaas-code-development
----
+
This tells OpenShift to build from your personal Gitea repository instead of the upstream GitHub repository.

* Save the file

* Commit these configuration changes to your Gitea repository:
+
[source,bash,subs="attributes"]
----
cd /projects/handson-llmaas-code-development
git add openshift/deployment.yaml
git add openshift/buildconfig.yaml
git commit -m "Configure deployment for {user}"
git push origin main
----
+
NOTE: If prompted for credentials during push, use your username (`{user}`) and password (`{password}`)

== Step 6: Commit Your Completed Code

Before deploying to OpenShift, you need to commit and push your completed application code to your Gitea repository. This ensures OpenShift builds the latest version of your code.

* Make sure you're in the repository root directory:
+
[source,bash]
----
cd /projects/handson-llmaas-code-development
----

* Check which files you've modified:
+
[source,bash]
----
git status
----
+
You should see `app.py` as modified.

* Stage and commit your completed code:
+
[source,bash,subs="attributes"]
----
git add app.py
git commit -m "Complete task manager application with AI integration"
----

* Push your changes to Gitea:
+
[source,bash]
----
git push origin main
----
+
NOTE: If prompted for credentials, use your username (`{user}`) and password (`{password}`)

* Verify the push was successful - you can check your Gitea repository in the browser to see the updated code

Now your completed application is ready to be built and deployed!

== Step 7: Build and Deploy to OpenShift

With your code complete, tested, and pushed to Gitea, it's time to deploy!

=== Create the ImageStream

An ImageStream tracks image updates in OpenShift.

[source,bash]
----
oc apply -f openshift/imagestream.yaml
----

=== Create the Build Configuration

This tells OpenShift how to build your container image from source.

[source,bash]
----
oc apply -f openshift/buildconfig.yaml
----

The BuildConfig will automatically trigger a build when created. You can monitor the build progress:

[source,bash]
----
oc logs -f bc/task-manager
----

This build will:

* Clone your code from the Gitea repository
* Build a container image using your Dockerfile
* Push the image to OpenShift's internal registry

The build will take 2-3 minutes. Watch for `Push successful` at the end.

=== Deploy the Application

Now deploy your application:

[source,bash]
----
oc apply -f openshift/deployment.yaml
oc apply -f openshift/service.yaml
oc apply -f openshift/route.yaml
----

=== Verify the Deployment

Check that your pod is running:

[source,bash]
----
oc get pods
----

You should see something like:

----
NAME                            READY   STATUS    RESTARTS   AGE
task-manager-7d9f4c5b6d-x8h2k   1/1     Running   0          30s
----

If the status shows `ImagePullBackOff` or `CrashLoopBackOff`, check the logs:

[source,bash]
----
oc logs -f deployment/task-manager
----

=== Get Your Application URL

Find your application's public URL:

[source,bash]
----
oc get route task-manager -o jsonpath='{.spec.host}'
----

Copy this URL and open it in your browser!

== Step 8: Test Your Deployed Application

Your AI-powered task manager is now live on OpenShift!

* Open the application URL in your browser

* Create several tasks:
** "Implement user authentication" (High priority)
** "Write unit tests" (Medium priority)
** "Update documentation" (Low priority)

* Click **AI Suggestions** on the authentication task
** You should see intelligent, specific suggestions from the AI model

* Try the analysis features:
** Click "üìä Prioritize Tasks" - AI will suggest which tasks to tackle first
** Click "üìã Task Overview" - Get a summary of all your tasks
** Click "‚è±Ô∏è Completion Estimate" - AI will estimate timelines

* Mark some tasks as completed and observe how the UI updates

== Step 9: Review the OpenShift Console

Let's see your application from the platform perspective.

* Open the OpenShift Console: https://console-openshift-console.{openshift_cluster_ingress_domain}/[https://console-openshift-console.{openshift_cluster_ingress_domain}/,window=_blank]

* Navigate to **Developer** perspective (top-left dropdown)

* Select your project: `{user}-task-manager`

* Click on **Topology** to see a visual representation of your application

You should see:

* Your `task-manager` deployment (represented as a blue circle)
* The route (indicated by an arrow icon in the top-right of the deployment)
* Build information if you click on the deployment

* Click on the deployment circle to open the details panel:
** **Resources** tab shows your pod, service, and route
** **Pods** section shows if your pod is healthy
** **Builds** section shows your build history

* Click **View logs** on your pod to see the application logs in real-time

== Step 10: Understanding the Architecture

Let's reflect on what you've built and how it works:

=== Application Architecture

Your task manager consists of:

* **Frontend**: A single-page application built with vanilla JavaScript and HTML
* **Backend**: Flask REST API with endpoints for CRUD operations
* **AI Integration**: Direct API calls to your Granite model deployed in Module 1

=== OpenShift Components

Your deployment uses:

* **BuildConfig**: Defines how to build your container from source
* **ImageStream**: Tracks your container image versions
* **Deployment**: Manages your application pods with health checks
* **Service**: Provides internal networking for your pods
* **Route**: Exposes your application to the internet with TLS

=== AI Integration Flow

When a user requests AI suggestions:

1. Frontend sends POST request to `/api/tasks/<id>/suggest`
2. Flask backend retrieves the task details
3. Backend constructs a prompt with task information
4. Backend calls `call_ai_model()` which sends a request to Granite
5. Granite processes the prompt and returns suggestions
6. Backend returns suggestions to frontend
7. Frontend displays the AI-generated suggestions

This is a **synchronous** AI integration pattern - simple but effective for low-latency requests.

== Reflection: What You've Accomplished

Let's take a moment to appreciate what you've achieved:

‚úÖ **AI-Assisted Development**: You used Continue.dev to complete a real application, experiencing modern development workflows

‚úÖ **Full-Stack Development**: You worked with Python, Flask, JavaScript, HTML, and REST APIs

‚úÖ **AI Integration**: You integrated an LLM into an application, handling prompts, API calls, and responses

‚úÖ **Container Building**: You created a production-ready Dockerfile for your application

‚úÖ **OpenShift Deployment**: You deployed to a production-grade Kubernetes platform with proper builds, deployments, and routing

‚úÖ **Cloud-Native Practices**: You used health checks, environment variables, and 12-factor app principles

This mirrors real enterprise development: AI models aren't just standalone services - they're integrated into applications that solve business problems.

== Optional Challenges

Want to go further? Try these enhancements:

=== Challenge 1: Add Task Persistence

Right now, tasks are stored in memory and disappear when the pod restarts.

* Ask Continue.dev: "How can I add SQLite database persistence to this Flask app?"
* Implement a simple database backend
* Redeploy and verify tasks persist across pod restarts

=== Challenge 2: Enhance the AI Prompts

The AI suggestions could be more sophisticated.

* Modify the prompts in `suggest_improvements()` to include more context
* Try different temperature values for creative vs. consistent responses
* Add system messages to guide the AI's behavior

=== Challenge 3: Add Authentication

Protect your task manager with user authentication.

* Use Continue.dev to add basic Flask-Login authentication
* Create user registration and login pages
* Associate tasks with specific users

=== Challenge 4: Improve Error Handling

Make the application more robust.

* Add try-except blocks around all API calls
* Return user-friendly error messages
* Add retry logic for AI model calls

== Troubleshooting

=== Application Won't Start

* Check pod logs: `oc logs deployment/task-manager`
* Verify environment variables in `deployment.yaml`
* Ensure the image built successfully: `oc get builds`

=== AI Suggestions Don't Work

* Verify the AI model service is accessible from your namespace
* Check the `AI_MODEL_URL` and `AI_API_TOKEN` environment variables
* Test the model endpoint directly:
+
[source,bash,subs="attributes"]
----
oc exec deployment/task-manager -- curl -X POST http://granite-instruct-vllm.{user}-ai-models.svc.cluster.local:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_TOKEN" \
  -d '{"model":"granite-3.3-8b-instruct","messages":[{"role":"user","content":"Hello"}],"max_tokens":50}'
----
+
NOTE: Replace `YOUR_API_TOKEN` with your actual API token

=== Build Failures

* Check build logs: `oc logs bc/task-manager`
* Verify the BuildConfig points to the correct repository and branch
* Ensure the `contextDir` is set to `exercises/05-code-development`

== What's Next?

You've successfully built and deployed an AI-powered application on OpenShift! You've experienced:

* The power of AI-assisted development with Continue.dev
* Full-stack application development with AI integration
* Modern DevOps practices with OpenShift

In the next module, you'll explore **Parasol AI Studio** - a specialized platform that makes enterprise AI accessible to everyone by eliminating the complexity of model configuration and deployment. You'll see how Parasol AI Studio transforms the AI application experience through automatic provisioning, seamless integration, and one-click access to powerful AI workbenches.

Your task manager is just the beginning - imagine what else you can build with these tools!
